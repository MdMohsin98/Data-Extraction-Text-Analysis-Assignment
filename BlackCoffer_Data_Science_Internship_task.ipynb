{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae3eebe",
   "metadata": {},
   "source": [
    "# Web Secrapping from the websites present in excel file and exporting it to another excel file after doing some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2385d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from pathlib import Path\n",
    "from newspaper import Config\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c1b90ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...\n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4       5  https://insights.blackcoffer.com/how-artificia..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "df = pd.read_csv(\"Input.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aeb8bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167</td>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168</td>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169</td>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170</td>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171</td>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "165     167  https://insights.blackcoffer.com/role-big-data...\n",
       "166     168  https://insights.blackcoffer.com/sales-forecas...\n",
       "167     169  https://insights.blackcoffer.com/detect-data-e...\n",
       "168     170  https://insights.blackcoffer.com/data-exfiltra...\n",
       "169     171  https://insights.blackcoffer.com/impacts-of-co..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6627b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NA values\n",
    "df=df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0b7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function of \"remove punctuation\" to remove the punctuations\n",
    "def remove_punc(dmf):\n",
    "    global nopunc\n",
    "    nopunc = [char for char in dmf if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a93a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function of \"remove stopwords\" to remove the stopwords\n",
    "def remove_stpwd(nopunc):\n",
    "    global clean_mess\n",
    "    clean_mess = [word for word in nopunc.split() if word.lower() not in stpwrd]\n",
    "    clean_mess=' '.join(clean_mess)\n",
    "    return clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae22ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to \"tokenize\" to tokenize the words\n",
    "def tokenize(clean_mess):\n",
    "    global tokens\n",
    "    tokens = word_tokenize(clean_mess.upper())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059122a",
   "metadata": {},
   "source": [
    "# 1. NEGATIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270cd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a neg_words() function to count the negative words present in the text\n",
    "def neg_words(tokens):\n",
    "    global num_neg\n",
    "    file = open('negdict.txt', 'r')\n",
    "    neg_words = file.read().split()\n",
    "    \n",
    "    num_neg=0\n",
    "    for n in range(len(tokens)):\n",
    "        words=str(tokens[n])\n",
    "        if words in neg_words:\n",
    "            num_neg=num_neg+1\n",
    "    return num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c712ef7",
   "metadata": {},
   "source": [
    "# 2. POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f6da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a pos_words() function to count the positive words present in the text\n",
    "def pos_words(tokens):\n",
    "    global num_pos\n",
    "    file = open('posdict.txt', 'r')\n",
    "    pos_words = file.read().split()\n",
    "    num_pos=0\n",
    "    for n in range(len(tokens)):\n",
    "        words=str(tokens[n])\n",
    "        if words in pos_words:\n",
    "            num_pos=num_pos+1\n",
    "    return num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b067146",
   "metadata": {},
   "source": [
    "# 3. POLARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0106325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Polarity Score\"\n",
    "def polarity_sc(num_pos, num_neg):\n",
    "    global polarity\n",
    "    polarity=(num_pos-num_neg)/((num_pos+num_neg)+0.000001)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f518da",
   "metadata": {},
   "source": [
    "# 4. SUBJECTIVITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbcaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Subjectivity Score\"\n",
    "def subjectivity_sc(num_pos, num_neg,tokens):\n",
    "    global subjectivity\n",
    "    subjectivity=(num_pos+num_neg)/(len(tokens)+0.000001)\n",
    "    return subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c56357",
   "metadata": {},
   "source": [
    "# 5. AVG SENTENCE LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80473d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Average Sentence Length\"\n",
    "def avgsent_len(dmf):\n",
    "    global avg_sent_len\n",
    "    text = dmf\n",
    "    sent=sent_tokenize(dmf)\n",
    "    avg_sent_len=(len(dmf.split()))/(len(sent))\n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6db1f",
   "metadata": {},
   "source": [
    "# 6. PERCENTAGE OF COMPLEX WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c21586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Percentage of Complex Words\"\n",
    "def complex_words_percent(dmf):\n",
    "    global complex_wrd\n",
    "    global complex_per\n",
    "    complex_wrd=0\n",
    "    wrd=dmf.split()\n",
    "    for i in wrd:\n",
    "        if len(i)>2:\n",
    "            complex_wrd=complex_wrd+1\n",
    "    complex_per=complex_wrd/len(wrd)\n",
    "    return complex_per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac8e59",
   "metadata": {},
   "source": [
    "# 7. FOG INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30601dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"fog_index\"\n",
    "def fog_index(avg_sent_len,complex_per):\n",
    "    global fogind\n",
    "    fogind=0.4*(avg_sent_len+complex_per)\n",
    "    return fogind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a6da3",
   "metadata": {},
   "source": [
    "# 8. AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0465852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Average no. of words per sentence\"\n",
    "def avg_wrds_in_sent(dmf):\n",
    "    global avg_wrds_per_sent\n",
    "    totwrd=len(dmf.split())\n",
    "    totsent=(len(sent_tokenize(dmf)))\n",
    "    avg_wrds_per_sent=totwrd/totsent\n",
    "             \n",
    "    return avg_wrds_per_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4c153",
   "metadata": {},
   "source": [
    "# 9. COMPLEX WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b1d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to count the \"Complex words\"\n",
    "def complex_wrd_count(dmf):\n",
    "    global complex_wrds\n",
    "    complex_wrds=0\n",
    "    words=dmf.split()\n",
    "    for i in words:\n",
    "        if len(i)>2:\n",
    "            complex_wrds=complex_wrds+1\n",
    "    return complex_wrds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4581c",
   "metadata": {},
   "source": [
    "# 10. WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552d5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to count the \"Total Cleaned Words\"\n",
    "def word_count(clean_mess):\n",
    "    global wrd_cnt\n",
    "    wrd_cnt=len((clean_mess.split()))\n",
    "    return wrd_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f722d72",
   "metadata": {},
   "source": [
    "# 11. SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28f2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to count the \"No. of Syllables\" present in the text\n",
    "def syllable_count(dmf):\n",
    "    global syl_count\n",
    "    text = dmf\n",
    "    syl_count = 0\n",
    "    vowels = \"aeiou\"\n",
    "    for word in text.lower().split():\n",
    "        #print(word)\n",
    "        if word[0] in vowels:\n",
    "            syl_count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                syl_count += 1\n",
    "        if word.endswith(\"ed\") or word.endswith(\"es\"):\n",
    "            syl_count -= 1\n",
    "    return syl_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202e0a8",
   "metadata": {},
   "source": [
    "# 12. PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36fdb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Personal Pronouns\"\n",
    "def personal_pronoun(dmf):\n",
    "    global personal_pro\n",
    "    pronounRegex = re.compile(r'\\bI\\b|\\bwe\\b|\\bWe\\b|\\bmy\\b|\\bMy\\b|\\bours\\b|\\bus\\b')\n",
    "    pps=pronounRegex.findall(dmf)\n",
    "    personal_pro=len(pps)\n",
    "    return personal_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0831f",
   "metadata": {},
   "source": [
    "# 13. AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4838ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the \"Average Words Length\"\n",
    "def avg_wrd_length(dmf):\n",
    "    global avg_wrdlen\n",
    "    alltxt=dmf\n",
    "    wrd_cnt=0\n",
    "    char_cnt=0\n",
    "    for word in alltxt:\n",
    "        if word!= ' ':\n",
    "            char_cnt=wrd_cnt+1\n",
    "    totalchar=char_cnt\n",
    "    for word in alltxt.split():\n",
    "        wrd_cnt=wrd_cnt+1\n",
    "    totalwrds=wrd_cnt\n",
    "    avg_wrdlen=totalchar/totalwrds\n",
    "    return avg_wrdlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c4f8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is necessary to increase the time out to prevent \"read timed out\" error\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = USER_AGENT\n",
    "config.request_timeout = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following for loop parses through each url in the csv file, downloads the title and the body\n",
    "# Then it finally appends the text in text file\n",
    "for n in range(len(df)):\n",
    "    urlm=df['URL'][n]\n",
    "    article=Article(urlm, config=config)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    art=article.title\n",
    "    art2=article.text\n",
    "    p = Path(str(df['URL_ID'][n])+'.txt')\n",
    "    p=  open(str(df['URL_ID'][n])+'.txt', 'a',encoding='utf-8')   \n",
    "    p.write(art)\n",
    "    p = open(str(df['URL_ID'][n])+'.txt', 'a',encoding='utf-8')\n",
    "    p.write('\\n \\n')\n",
    "    p.write(art2)\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7408fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrd = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bce94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2=open('StopWords_Auditor.txt')\n",
    "file2=file2.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "088d286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add additional stopwords to the corpus using .extend() method as follows\n",
    "stpwrd.extend(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e4227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_dict=pd.read_csv('LoughranMcDonald_MasterDictionary_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a093577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have created a positive word and a negative word dictionary using the positive and negative score\n",
    "# from the mst_dict respectively\n",
    "mst_dict['neg_wrd'] = mst_dict.loc[mst_dict['Negative'] == 2009]['Word']\n",
    "mst_dict['pos_wrd'] = mst_dict.loc[mst_dict['Positive'] == 2009]['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aae6b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(mst_dict['neg_wrd'])):\n",
    "    word=str(mst_dict['neg_wrd'][n])\n",
    "    if word!='nan':\n",
    "        p=open('negdict.txt','a')\n",
    "        p.write(word +' ')\n",
    "\n",
    "for n in range(86531):\n",
    "    word=str(mst_dict['pos_wrd'][n])\n",
    "    if word!='nan':\n",
    "        p=open('posdict.txt','a')\n",
    "        p.write(word +' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acb7a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating a dictionary that will contain all the extracted values of the input file\n",
    "alldata={ 'URL_ID':[],\n",
    "          'URL':[],\n",
    "          'POSITIVE SCORE':[],\n",
    "          'NEGATIVE SCORE':[],\n",
    "          'POLARITY SCORE':[],\n",
    "          'SUBJECTIVITY SCORE':[],\n",
    "          'AVG SENTENCE LENGTH':[],\n",
    "          'PERCENTAGE OF COMPLEX WORDS':[],\n",
    "          'FOG INDEX':[],\n",
    "          'AVG NUMBER OF WORDS PER SENTENCE':[],\n",
    "          'COMPLEX WORD COUNT':[],\n",
    "          'WORD COUNT':[],\n",
    "          'SYLLABLE PER WORD':[],\n",
    "          'PERSONAL PRONOUNS':[],\n",
    "          'AVG WORD LENGTH':[]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ffdb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally I am writing the main function\n",
    "for n in range(len(df)):\n",
    "    file=open(str(df['URL_ID'][n])+'.txt', 'r',encoding='utf-8')\n",
    "    dmf=file.read()\n",
    "    l=df['URL'][n]\n",
    "    alldata['URL'].append(l)\n",
    "    remove_punc(dmf)\n",
    "    remove_stpwd(nopunc)\n",
    "    tokenize( clean_mess)\n",
    "    neg_words(tokens)\n",
    "    pos_words(tokens)\n",
    "    polarity_sc(num_pos, num_neg)\n",
    "    subjectivity_sc(num_pos, num_neg,tokens)\n",
    "    avgsent_len(dmf)\n",
    "    complex_words_percent(dmf)\n",
    "    fog_index(avg_sent_len,complex_per)\n",
    "    avg_wrds_in_sent(dmf)\n",
    "    complex_wrd_count(dmf)\n",
    "    word_count(clean_mess)\n",
    "    syllable_count(dmf)\n",
    "    personal_pronoun(dmf)\n",
    "    avg_wrd_length(dmf) \n",
    "    alldata['URL_ID'].append(n)   \n",
    "    alldata['NEGATIVE SCORE'].append(num_neg)\n",
    "    alldata['POSITIVE SCORE'].append(num_pos)\n",
    "    alldata['POLARITY SCORE'].append(polarity)\n",
    "    alldata['SUBJECTIVITY SCORE'].append(subjectivity)\n",
    "    alldata['AVG SENTENCE LENGTH'].append(avg_sent_len)\n",
    "    alldata['PERCENTAGE OF COMPLEX WORDS'].append(complex_per)\n",
    "    alldata['FOG INDEX'].append(fogind)\n",
    "    alldata['AVG NUMBER OF WORDS PER SENTENCE'].append(avg_wrds_per_sent)\n",
    "    alldata['COMPLEX WORD COUNT'].append(complex_wrds)\n",
    "    alldata['WORD COUNT'].append(wrd_cnt)\n",
    "    alldata['SYLLABLE PER WORD'].append(syl_count)\n",
    "    alldata['PERSONAL PRONOUNS'].append(personal_pro)\n",
    "    alldata['AVG WORD LENGTH'].append(avg_wrdlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ee0c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export the data into excel file, I have converted the whole data into pandas dataframe\n",
    "exceldata=pd.DataFrame(alldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61303f35",
   "metadata": {},
   "source": [
    "# Finally I have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31dd52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the final data into new excel file\n",
    "file_name = 'Output_with_completing_tasks.csv'\n",
    "exceldata.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ac6d7",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
